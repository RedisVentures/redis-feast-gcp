{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jirdTjhETQW0"
   },
   "source": [
    "# **redis-feast-gcp**: 01 - Triton Model Training\n",
    "\n",
    "In this notebook, we will use the Feast SDK to pull historical data for training an `XGBoost` regression model. We'll then deploy that model to the NVIDIA Triton Inference Server model repository.\n",
    "\n",
    "**This notebook assumes that you've already set up your Feature Store and model repo in GCP.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](img/redis-feast-gcp-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cJFAJiuGxM3"
   },
   "source": [
    "# Fetching Historical Data\n",
    "\n",
    "Similar to our first notebook, we can use Feast to prepare an accurate training dataset across several different feature views (tables).\n",
    "\n",
    "To make things simpler, we use the [`DataFetcher`](utils/data_fetcher.py) class that wraps Feast, which wraps Redis and BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_store.repo import config\n",
    "from feature_store.utils import (\n",
    "    DataFetcher,\n",
    "    storage\n",
    ")\n",
    "\n",
    "# Load fs\n",
    "fs = storage.get_feature_store(\n",
    "    config_path=config.REPO_CONFIG,\n",
    "    bucket_name=config.BUCKET_NAME\n",
    ")\n",
    "\n",
    "# Load data fetcher\n",
    "data_fetcher = DataFetcher(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "BqgiEP2Oz42q",
    "outputId": "40552317-644b-4ee6-d735-0ee5e48e79dd"
   },
   "outputs": [],
   "source": [
    "# Fetch Historical Training Data\n",
    "ds = data_fetcher.get_training_data(\n",
    "    entity_query=f\"\"\"\n",
    "        select\n",
    "            state,\n",
    "            date as event_timestamp\n",
    "        from\n",
    "            {config.BIGQUERY_DATASET_NAME}.{config.WEEKLY_VACCINATIONS_TABLE}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Clean up any nulls\n",
    "ds.dropna(inplace=True)\n",
    "ds.sort_values(['event_timestamp', 'state'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Prep and Analysis\n",
    "\n",
    "We won't do too much here. The point is to demonstrate the correlation between our features and the target value (`weekly_vaccinations_count`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a subset of data\n",
    "virginia = ds[ds.state == \"Virginia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginia.plot.scatter(y='weekly_vaccinations_count', x='lag_2_vaccine_interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginia.plot.scatter(y='weekly_vaccinations_count', x='lag_2_vaccine_safety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginia.plot.scatter(y='weekly_vaccinations_count', x='lag_2_vaccine_intent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virginia.plot.scatter(y='weekly_vaccinations_count', x='lag_1_weekly_vaccinations_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3izkr_3sG1hX"
   },
   "source": [
    "# Model Training\n",
    "\n",
    "Use loaded features to train an XGboost regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import timedelta\n",
    "from xgboost import XGBRegressor\n",
    "from feature_store.repo import config\n",
    "from feature_store.utils import (\n",
    "    DataFetcher,\n",
    "    logger,\n",
    "    storage\n",
    ")\n",
    "\n",
    "logging = logger.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    data: pd.DataFrame,\n",
    "    n: int,\n",
    "    timestamp_col: str,\n",
    "    by: str = \"week\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Split a timeseries dataset into train/test sets by date.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): _description_\n",
    "        n (int): _description_\n",
    "        timestamp_col (str): _description_\n",
    "        by (str, optional): _description_. Defaults to \"week\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If \"by\" arg is not one of \"week\" or \"day\".\n",
    "    \"\"\"\n",
    "    if by == \"week\":\n",
    "        delta = timedelta(weeks=n)\n",
    "    elif by == \"day\":\n",
    "        delta == timedelta(days=n)\n",
    "    else:\n",
    "        raise ValueError(\"'by' must be one of 'week' or day'\")\n",
    "    # Split the data\n",
    "    split_point = data[timestamp_col].max() - delta\n",
    "    train = data[data[timestamp_col] <= split_point]\n",
    "    test = data[data[timestamp_col] > split_point]\n",
    "    train.drop(columns=[timestamp_col, \"state\"], axis=1, inplace=True)\n",
    "    test.drop(columns=[timestamp_col, \"state\"], axis=1, inplace=True)\n",
    "    logging.info(f\"Training Results: {len(train)} samples in the training set\")\n",
    "    return train, test\n",
    "\n",
    "def xgboost_train(train: pd.DataFrame, test: pd.DataFrame = None):\n",
    "    \"\"\"\n",
    "    Train an XGBoost.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training DataFrame.\n",
    "        test (pd.DataFrame): Testing DataFrame. (Optional)\n",
    "    \"\"\"\n",
    "    # split into input and output columns\n",
    "    train = train.to_numpy()\n",
    "    trainX, trainy = train[:, :-1], train[:, -1]\n",
    "\n",
    "    # make xgboost regressor model\n",
    "    model = XGBRegressor(\n",
    "        random_state=42,\n",
    "        objective=\"count:poisson\",\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mae\",\n",
    "        n_estimators=250,\n",
    "        max_depth=4\n",
    "    )\n",
    "    if isinstance(test, pd.DataFrame):\n",
    "        test = test.to_numpy()\n",
    "        testX, testy = test[:, :-1], test[:, -1]\n",
    "        return model.fit(\n",
    "            trainX,\n",
    "            trainy,\n",
    "            eval_set=[(testX, testy)]\n",
    "        )\n",
    "    return model.fit(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init feature store and data fetcher\n",
    "logging.info(\"Loading feature store and data fetcher\")\n",
    "\n",
    "store = storage.get_feature_store(\n",
    "    config_path=config.REPO_CONFIG,\n",
    "    bucket_name=config.BUCKET_NAME\n",
    ")\n",
    "\n",
    "data_fetcher = DataFetcher(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train/Test split for validation\n",
    "train, test = train_test_split(\n",
    "    data=ds,\n",
    "    n=1,\n",
    "    timestamp_col=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "# Train initial model\n",
    "model = xgboost_train(\n",
    "    train[data_fetcher.X_cols + data_fetcher.y_col],\n",
    "    test[data_fetcher.X_cols + data_fetcher.y_col]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Initial model training complete. Validation loss observed.\")\n",
    "\n",
    "# Train final model\n",
    "model = xgboost_train(ds[data_fetcher.X_cols + data_fetcher.y_col])\n",
    "logging.info(\"Final model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model to the local Triton Model Repo\n",
    "model_version = 1\n",
    "model_path = f\"./setup/models/{config.MODEL_NAME}/{model_version}/{config.MODEL_FILENAME}\"\n",
    "model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore feature importance\n",
    "for feature, imp in zip(data_fetcher.X_cols, model.feature_importances_):\n",
    "    print(feature, imp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Triton Model Repository\n",
    "\n",
    "This E2E example uses **GCP Cloud Storage** for the **Triton Model Repository**. Most likely, you've already run the setup container which would have generated the initial model repository in your GCP project. Now that we have a \"new\" model, we can deploy and update the model in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "remote_model_path = f\"models/{config.MODEL_NAME}/{model_version}/{config.MODEL_FILENAME}\"\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(config.BUCKET_NAME)\n",
    "blob = bucket.blob(remote_model_path)\n",
    "blob.upload_from_filename(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the whole model repo spelled out\n",
    "[b for b in bucket.list_blobs() if \"models\" in b.name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you restart your Vertex model in the GCP dashboard -- you should get the latest models from GCS copied to your Triton container running in Vertex\n",
    "\n",
    "*Since Triton version 22.07 there is a bug in accessing GCS as the model repo. This will be fixed in v23.01 and we will update the example accordingly.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fraud Detection Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1c5a7c9cc0d58080444e081b74a0823c09a12f0209aca730c38726ea6940124"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
